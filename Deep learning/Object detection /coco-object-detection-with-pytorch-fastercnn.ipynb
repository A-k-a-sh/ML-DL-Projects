{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"242a7c9c-7174-4467-add0-ea91a9f09dfd","_cell_guid":"a0c9dbe2-d8f7-4145-96cd-52c474132930","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-07T08:44:06.118259Z","iopub.execute_input":"2025-11-07T08:44:06.118656Z","iopub.status.idle":"2025-11-07T08:44:06.123404Z","shell.execute_reply.started":"2025-11-07T08:44:06.118623Z","shell.execute_reply":"2025-11-07T08:44:06.122261Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"root_datapath = '/kaggle/input/coco-2017-dataset/coco2017'","metadata":{"_uuid":"7fb89a80-d5b0-4ae1-a391-9c63158aa643","_cell_guid":"94981840-5e3e-4ba2-a430-5e82a4e09962","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-07T08:44:06.124575Z","iopub.execute_input":"2025-11-07T08:44:06.124796Z","iopub.status.idle":"2025-11-07T08:44:06.145883Z","shell.execute_reply.started":"2025-11-07T08:44:06.124777Z","shell.execute_reply":"2025-11-07T08:44:06.144939Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfor *_, filenames in os.walk(f'{root_datapath}/annotations'):\n    for filename in filenames:\n        print(filename)","metadata":{"_uuid":"1f3bd93a-0fe0-42cd-a340-8352dc5e5db8","_cell_guid":"5063bdce-b419-4abb-8c88-10913a946666","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-07T08:44:06.147170Z","iopub.execute_input":"2025-11-07T08:44:06.147488Z","iopub.status.idle":"2025-11-07T08:44:06.178901Z","shell.execute_reply.started":"2025-11-07T08:44:06.147454Z","shell.execute_reply":"2025-11-07T08:44:06.177937Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\n# Path to annotation file\nann_file = f\"{root_datapath}/annotations/instances_train2017.json\"\n\n# Load JSON\nwith open(ann_file, 'r') as f:\n    coco_data = json.load(f)","metadata":{"_uuid":"6f2a2608-f8ea-4ccd-823f-181348bed3b4","_cell_guid":"4dd380d0-bb13-481a-b729-a7aa7a6be592","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-07T08:44:06.180340Z","iopub.execute_input":"2025-11-07T08:44:06.180662Z","iopub.status.idle":"2025-11-07T08:44:30.084834Z","shell.execute_reply.started":"2025-11-07T08:44:06.180634Z","shell.execute_reply":"2025-11-07T08:44:30.083812Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#id to cat\ncoco_id_map = {cat['id']: cat['name'] for cat in coco_data['categories']}\nprint(coco_id_map.get(333))","metadata":{"_uuid":"a268f481-9512-4bfb-b062-dcb1d341e689","_cell_guid":"fe5703d3-fdef-4869-9af2-2185020babc0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-07T08:44:30.085790Z","iopub.execute_input":"2025-11-07T08:44:30.086077Z","iopub.status.idle":"2025-11-07T08:44:30.091442Z","shell.execute_reply.started":"2025-11-07T08:44:30.086054Z","shell.execute_reply":"2025-11-07T08:44:30.090311Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import defaultdict\n\ncategory_name = 'hot dog'\ncategory_list = coco_data[\"categories\"]\ncategory_id = next(cat[\"id\"] for cat in category_list if cat[\"name\"] == category_name)\n\n# --- Group all boxes for each image ---\nimg_to_bboxes = defaultdict(list)\nfor ann in coco_data[\"annotations\"]:\n    if ann[\"category_id\"] == category_id:\n        img_to_bboxes[ann[\"image_id\"]].append(ann[\"bbox\"])\n\n\nfrom collections import defaultdict\n\nimg_to_anns = defaultdict(list)\nfor ann in coco_data['annotations']:\n    img_to_anns[ann['image_id']].append(ann)\n\n\nfrom pathlib import Path\n\ndef get_annotations(img_path: Path) -> dict:\n    img_file_name = img_path.stem + '.jpg'\n    img_info = next((img for img in coco_data['images'] if img['file_name'] == img_file_name), None)\n    if img_info is None:\n        return {\"boxes\": [], \"labels\": []}\n\n    anns = img_to_anns.get(img_info['id'], [])\n    boxes_list = [ann['bbox'] for ann in anns]\n    \n    labels_list = [ann['category_id'] for ann in anns]\n\n    return {\"boxes\": boxes_list, \"labels\": labels_list}","metadata":{"_uuid":"d4905538-4873-446c-b4c1-f29d0e0dac75","_cell_guid":"03aedb0c-68ff-43e4-a998-74d02ec0971c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-07T08:44:30.093386Z","iopub.execute_input":"2025-11-07T08:44:30.093615Z","iopub.status.idle":"2025-11-07T08:44:33.261399Z","shell.execute_reply.started":"2025-11-07T08:44:30.093598Z","shell.execute_reply":"2025-11-07T08:44:33.260530Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport glob\nfrom pathlib import Path\ntarget_dir_train = Path(f'{root_datapath}/train2017')\ntarget_dir_test = Path(f'{root_datapath}/test2017')\n\ntrain_dir_path = sorted(list(target_dir_train.glob('*.jpg')))\ntest_dir_path = sorted(list(target_dir_test.glob('*.jpg')))\n\n# train_dir_path[:5] ,train_dir_path[1].stem","metadata":{"_uuid":"f73b2b19-cf41-4a15-9902-644cddd32fa8","_cell_guid":"bbdb20dc-8772-4dd6-9d09-ca2db8c4f005","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-07T08:44:33.262261Z","iopub.execute_input":"2025-11-07T08:44:33.262523Z","iopub.status.idle":"2025-11-07T08:44:34.937340Z","shell.execute_reply.started":"2025-11-07T08:44:33.262503Z","shell.execute_reply":"2025-11-07T08:44:34.936299Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import tv_tensors\nimport torch\n\n\ndef change_box(boxes, labels, img_tvtensor):\n    if boxes is None or len(boxes) == 0:\n        return (tv_tensors.BoundingBoxes(\n                    data=torch.zeros((0, 4), dtype=torch.float32),\n                    format=\"XYXY\",\n                    canvas_size=img_tvtensor.shape[-2:]\n                ),\n                torch.zeros((0,), dtype=torch.int64)\n        )\n\n    box_xywh = torch.tensor(boxes, dtype=torch.float32)\n    box_xyxy = box_convert(box_xywh, in_fmt='xywh', out_fmt='xyxy')\n    labels = torch.tensor(labels, dtype=torch.int64)\n\n    return tv_tensors.BoundingBoxes(\n        data=box_xyxy,\n        format=\"XYXY\",\n        canvas_size=img_tvtensor.shape[-2:]\n    ), labels\n\n\n\nfrom torchvision.transforms import v2\n\n\ntest_transform =  v2.Compose([ \n    v2.ToImage(),\n    v2.ToDtype(torch.float32, scale=True), \n])","metadata":{"_uuid":"3daa081f-c1ac-438f-b2ac-fc29dc10174d","_cell_guid":"84e02f7f-c096-46dd-8c64-afb46989af93","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-07T09:38:45.224513Z","iopub.execute_input":"2025-11-07T09:38:45.225524Z","iopub.status.idle":"2025-11-07T09:38:45.233032Z","shell.execute_reply.started":"2025-11-07T09:38:45.225493Z","shell.execute_reply":"2025-11-07T09:38:45.232147Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.ops import box_convert\nfrom torch.utils.data import Dataset\nfrom torchvision import tv_tensors\nfrom PIL import Image\nimport torch\n\n\nclass detectionDatasetCustom(Dataset):\n    def __init__(self , directory_path , transform = None):\n        self.paths = directory_path\n        self.transform = transform\n        self.annotations = [get_annotations(p) for p in self.paths] #if working with small dataset(few thousand images) | other wise ram will be blown\n\n    def __len__(self):\n        return len(self.paths)\n\n    def load_img(self ,img_path):\n        return Image.open(img_path).convert('RGB')\n\n\n\n    def __getitem__ (self , idx):\n        img_path = self.paths[idx]\n        # ann = get_annotations(img_path)\n        ann = self.annotations[idx] #for small dataset\n\n        img = self.load_img(img_path)\n\n        img_tvtensor = tv_tensors.Image(img)\n\n        boxes , labels = ann['boxes'] , ann['labels']\n\n\n        # box = change_box(boxes, img_tvtensor)\n        box, labels = change_box(ann['boxes'], ann['labels'], img_tvtensor)\n\n\n        # target = {'boxes' : box, 'labels' : torch.tensor(labels , dtype = torch.int64)}\n        target = {'boxes': box, 'labels': labels}\n\n\n\n        if self.transform:\n\n            img_tvtensor , target = self.transform(img_tvtensor , target)\n\n            \n        return img_tvtensor , target","metadata":{"_uuid":"95f7ab53-3cf4-4567-94d5-a75b8ace7121","_cell_guid":"6b355f04-0ed4-4940-8f78-baf634078770","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-07T10:05:53.282384Z","iopub.execute_input":"2025-11-07T10:05:53.282766Z","iopub.status.idle":"2025-11-07T10:05:53.292518Z","shell.execute_reply.started":"2025-11-07T10:05:53.282745Z","shell.execute_reply":"2025-11-07T10:05:53.291664Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"_uuid":"14851860-d5a2-44a1-93d9-e0161b0542ac","_cell_guid":"2ca09271-4439-4804-a618-fbd55fbe11f6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-07T08:44:34.970956Z","iopub.execute_input":"2025-11-07T08:44:34.971279Z","iopub.status.idle":"2025-11-07T08:44:34.992984Z","shell.execute_reply.started":"2025-11-07T08:44:34.971255Z","shell.execute_reply":"2025-11-07T08:44:34.992101Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.utils import draw_bounding_boxes\n\ndef prdicion_with_real(predictions, img_tensor , target = None, threshold=0.5):\n\n    plt.figure(figsize=(16, 10))\n    img = img_tensor\n    i = 1\n    if target is not None:\n        \n        # Ground truth boxes & labels\n        real_boxes = target['boxes'].to('cpu') if 'boxes' in target else torch.zeros((0, 4))\n        real_labels = target['labels'].to('cpu') if 'labels' in target else torch.zeros((0,), dtype=torch.int64)\n        real_labels_names = [coco_id_map[val.item()] for val in real_labels]\n\n        # Draw boxes\n        img_with_boxes_actual = draw_bounding_boxes(\n            image=img,\n            boxes=real_boxes,\n            labels=real_labels_names,\n            colors='lime',\n            width=3,\n            # fill_labels=True,\n        )\n\n        plt.subplot(1, 2, i)\n        i += 1\n        plt.imshow(img_with_boxes_actual.permute(1, 2, 0))\n        plt.axis('off')\n        plt.title('Ground Truth (lime)')\n    \n    \n    img_bbox_tuple = []\n    \n    for pred in predictions:\n        boxes, labels, scores = [], [], []\n        for bbox, lb, sc in zip(pred['boxes'], pred['labels'], pred['scores']):\n            if sc >= threshold and coco_id_map.get(lb.item()) is not None:\n                boxes.append(bbox.to('cpu'))\n                scores.append(sc.to('cpu'))\n                labels.append(lb.to('cpu'))\n        img_bbox_tuple.append((boxes, labels, scores))\n\n\n    # Predictions\n    pred_boxes, pred_labels, scores = img_bbox_tuple[0]\n\n    pred_boxes = torch.stack(pred_boxes) if len(pred_boxes) > 0 else torch.zeros((0, 4), dtype=torch.float)\n\n    pred_labels_name = [\n        f\"{coco_id_map[val.item()]} {sc.item()*100:.0f}%\"\n        for val, sc in zip(pred_labels, scores)\n    ]\n\n    print(pred_labels_name)\n\n    img_with_boxes_pred = draw_bounding_boxes(\n        image=img,\n        boxes=pred_boxes,\n        labels=pred_labels_name,\n        colors='red',\n        width=3,\n        # fill_labels=True,\n    )    \n\n    plt.subplot(1, 2, i)\n    plt.imshow(img_with_boxes_pred.permute(1, 2, 0))\n    plt.axis('off')\n    plt.title('Predictions (red, with scores)')\n\n    plt.show()","metadata":{"_uuid":"7965f835-bf4e-41d9-9649-99f1542d4952","_cell_guid":"a50eb4a3-aaef-4c46-ba5f-2f3976b269b1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-07T10:13:19.118377Z","iopub.execute_input":"2025-11-07T10:13:19.119011Z","iopub.status.idle":"2025-11-07T10:13:19.138202Z","shell.execute_reply.started":"2025-11-07T10:13:19.118969Z","shell.execute_reply":"2025-11-07T10:13:19.135970Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchvision\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1)\nmodel.to(device)","metadata":{"_uuid":"43a29d03-9e36-40a9-8c09-d04375cbae6b","_cell_guid":"68dbdf23-4fad-4b2f-a7c6-7b635990a54e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-07T08:55:34.953972Z","iopub.execute_input":"2025-11-07T08:55:34.954282Z","iopub.status.idle":"2025-11-07T08:55:35.663205Z","shell.execute_reply.started":"2025-11-07T08:55:34.954258Z","shell.execute_reply":"2025-11-07T08:55:35.662101Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport random\n\n\ntrain_dataset_plot = detectionDatasetCustom(random.sample(train_dir_path, 5), test_transform)\nfor data in train_dataset_plot:\n    img_tensor , target = data\n    model.eval()\n    with torch.inference_mode():\n        predictions = model(img_tensor.unsqueeze(dim = 0).to(device))\n    prdicion_with_real( predictions , img_tensor, target)","metadata":{"_uuid":"99317860-e015-424b-b9ee-84d2df400acd","_cell_guid":"cf48ff5c-ceb8-45e0-823b-4e022b3a2d08","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-07T10:09:20.103416Z","iopub.execute_input":"2025-11-07T10:09:20.104221Z","iopub.status.idle":"2025-11-07T10:09:47.525060Z","shell.execute_reply.started":"2025-11-07T10:09:20.104179Z","shell.execute_reply":"2025-11-07T10:09:47.524118Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## detecting obj from a random downloaded img","metadata":{}},{"cell_type":"code","source":"import requests\nfrom pathlib import Path\n\ndownloaded_img = Path('Img')\ndownloaded_img.mkdir(parents = True, exist_ok= True)\n\nimg_path = downloaded_img /'my_img.jpg'\n\nwith open( img_path, 'wb') as f:\n    req = requests.get('https://images.squarespace-cdn.com/content/v1/574512d92eeb81676262d877/1652159560047-7ECWQB6E0GRW2WXVKLKZ/LosAngeles2022-88.jpg?format=2500w')\n    f.write(req.content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T10:12:04.573479Z","iopub.execute_input":"2025-11-07T10:12:04.573824Z","iopub.status.idle":"2025-11-07T10:12:04.767521Z","shell.execute_reply.started":"2025-11-07T10:12:04.573799Z","shell.execute_reply":"2025-11-07T10:12:04.766742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimg = Image.open(img_path).convert('RGB')\nimg\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T10:12:04.926965Z","iopub.execute_input":"2025-11-07T10:12:04.927368Z","iopub.status.idle":"2025-11-07T10:12:05.692890Z","shell.execute_reply.started":"2025-11-07T10:12:04.927344Z","shell.execute_reply":"2025-11-07T10:12:05.691800Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimg_transformed = test_transform(img)\n\nmodel.eval()\nwith torch.inference_mode():\n    pred = model(img_transformed.unsqueeze(dim = 0).to(device))\nprdicion_with_real(pred , img_transformed)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T10:13:25.016437Z","iopub.execute_input":"2025-11-07T10:13:25.016735Z","iopub.status.idle":"2025-11-07T10:13:32.039590Z","shell.execute_reply.started":"2025-11-07T10:13:25.016715Z","shell.execute_reply":"2025-11-07T10:13:32.038477Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}