{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nh2R60vnGZ7F",
        "outputId": "a1b01194-637d-49c8-c8dc-74c56390af45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Colab cache for faster access to the 'facial-emotion-recognition-dataset' dataset.\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "fahadullaha_facial_emotion_recognition_dataset_path = kagglehub.dataset_download('fahadullaha/facial-emotion-recognition-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D8yY9O6NYv5I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yzxO4j0bGnLz",
        "outputId": "a57fac91-b931-4c8a-f9f8-82d0fb767df7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/kaggle/input/facial-emotion-recognition-dataset'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "root_path = fahadullaha_facial_emotion_recognition_dataset_path\n",
        "root_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hpaZaoB_Ypbs",
        "outputId": "311a0e9b-381e-49a2-d579-382773757b7b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "DnINsthrGZ7H",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "# import os\n",
        "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#     for filename in filenames:\n",
        "#         print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "id": "HsEbe8M6GZ7I",
        "outputId": "3bd13e61-eecc-4d50-e368-f8d2316352c4",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABgAGADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDxO2YmYsxwcVoWdlPf3ICKSmeSBUf9nSSBmVsnHAArvfhu1tOHtLiICdBkhhzQBqeH9GhiRcqTXaQ6Zb+XwOO4pI7WKL/VrtBq/AOMDp60AQwaZHE4Mfyk1fWFlGGO6lRcNnFXI/mFAEUMKu+0rgVowwxbcKMGq4TDbh1qQyEL6UATNHGvXk1m3UXmEgjK+lT+d75qF5Pmz3oA8tu4yl3dhx8m44FclFaGe5kRhzng16d4m0Z/N+22y7o8fOo9a4OaDybkl1JU9R6UASeFdDiN0JmG8A8g1c8RWLeHNdt9ZtFxHOwWQDoBU/hPcIg6t8pPSus1DTo9X0ma1YBmKYT2NAFm2u4ru1W5jIMbjginxX8SShN4xXD+GL2XTmuNGuWJaDO3PetP7TA0jKxKuPXtQB38M0DgDzF596nRSp4ORXmVxbX0afaLO5Lgc4zWpofjaOPFtqLFHBx8woA71eDTZDxiqsV9FdRiSGVXQ+lOaXIoAUYU9ahc/OfSmmQDnNMEgJyKAHOQYyv8J65rldV0FJ5DLGOfpXVfLtJbpWXqGoW8AwzhU9c0Acd4ag8mDBGDnpXZWZVWDD7wrldOkUPjoa6SBjsG0ZHagDB8X6HetcR6rokHmXaHMig4zXApe6/rGoyPJAbd4T+9QdzXtdvLt4bv1rAbwveW17cXVi63CXBy8WMYoA5L+0LuwtvPbMYXr3rIutbttbBEmRKOjBcV2dzpkqswltjGe643Cs5NEtUl7qCckeXQBl6NqWsaMS0E7zQ/3CK6S28fruVb+BoSTgsQcCrEKWNvCRguVH9ysS8s9S8TE2en2flW/RpSuMCgD0O1lt723W4iuomiYf3xVK91+w09tplRiPQg15xq/wAPr3w5or3MGvS7lGTGM81neHdOV7Q3VzcNLKwzhmoA67U/HZmYxWanHTdXPXt+92mZrssT/ABTL+z+0/Lb4h9alttHWOEeZNlvUigDYiDRTgjpXTWk5WJRmuagk3rWrbTgpt7igDoIpkB5NXVuFAGGxXK/aShJJ4qCTW33bE/nQB2L6jHGD5jJj3ANUL3XbaEDbbLICcZxXOG9hA+dzJIe2elL9uiMRjcA+goA2pbl1QT/AGZQrfStjTLrzoQPLVV9hiuQg1OJ1FoX2tng10+k/Im126d6ADW7aGWyfzQCmOQa86sdIs2vJSh8tc8c12niO+/0d41PbrXFWzGN927nvQBsrolsTkyZFZetiKzh2w5L+grRn1IQW4J+ZjwoFaOm6Pbx2Ru9WXfcucpGf7tAHJrdLbrudsAVPaamlw+VOF9a5e8nac+UTkDnNWdNHO1/lUe9AG9fXzMxii5Pds1TMrwp8zb3P6VZhtkkO88Rirx02I7HjHyHqaAMcXV1CnmPanHYg5pF1iN+CCr+9b0sRttvO2Mj7pXrVP8Asu0lLSzlVz2FAEVqkV5ETC371fmznpVuw8YpZN9lum8wKeHq14b0WAzTvHnbsOSTWR/YenNqskxOQpztoAvar4jtryIrEcse9Y+n3MjTeVsMjN0AqO+W1Fy5ji2+2a6Tw7Bplhps+p3V3Gk8f+rQkZoA2dN0Gx0wpqOtTjzMZjg68VJfX8est52nAqqfKGI4FY667Ytp888s3myOcrjnHtiqljqt5qDm0hi+x2h5Jx1PrQBwMuRLub71OjufmVd3FQXjuZ/3gwMVHDwd449qAPQLBreXTxDv2uw4qFpbuzie1cHy371zlheFZF3OV/2q6qK585FT7/8AtGgCa111zEttfwCeBRhT0IFO83R3fcLRgey76qy21uULEbGHes8Bt+fNwg9qANa71mZLfyLSHyF9QeorAnvWiRnJy7dakvZFjTc0uQPasCa8aaQ7PuL29aAIb7VJI1wj5du/pWSWklcGaVyT6McflXY6b4GvPEsBns3wR94elbTfCgaXpMl9d3fmSJzsx0oAyfh3o9zPevdmNmiTP3+h/Ou3s4IY7iUzfI244GKi8OTCO2REARQOQB1rT1aTEASKMbjzvoA//9k=",
            "image/png": "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",
            "text/plain": [
              "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=96x96>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from PIL import Image\n",
        "\n",
        "img = Image.open(f'{root_path}/processed_data/angry/angry_00000.jpg')\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wahVx-okHWWj"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets\n",
        "\n",
        "def make_dataset(transform):\n",
        "  full_dataset = datasets.ImageFolder(\n",
        "    root = f'{root_path}/processed_data',\n",
        "    transform = transform,\n",
        "\n",
        "  )\n",
        "  torch.manual_seed(42)\n",
        "  # train_dataset , test_dataset = random_split(full_dataset , [0.8, 0.2])\n",
        "  train_len = int(0.8 * len(full_dataset))\n",
        "  test_len = len(full_dataset) - train_len\n",
        "  train_dataset, test_dataset = random_split(full_dataset, [train_len, test_len])\n",
        "\n",
        "\n",
        "  return train_dataset, test_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RWfMDdx2Q0jo",
        "outputId": "2064c6a4-c17e-41d0-8ad5-632c7dca32c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 139MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MobileNetV2(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2dNormActivation(\n",
              "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU6(inplace=True)\n",
              "    )\n",
              "    (1): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (2): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (3): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (4): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (5): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (6): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (7): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (8): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (9): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (10): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (11): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (12): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (13): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (14): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (15): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (16): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (17): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (18): Conv2dNormActivation(\n",
              "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU6(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.2, inplace=False)\n",
              "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "torch.manual_seed(42)\n",
        "\n",
        "model = models.mobilenet_v2(weights = models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rcziV4UDR1wy",
        "outputId": "470b242e-c6a1-4cbc-a4bd-816b2760cf74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MobileNetV2(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2dNormActivation(\n",
              "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU6(inplace=True)\n",
              "    )\n",
              "    (1): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (2): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (3): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (4): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (5): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (6): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (7): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (8): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (9): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (10): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (11): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (12): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (13): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (14): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (15): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (16): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (17): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (18): Conv2dNormActivation(\n",
              "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU6(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.2, inplace=False)\n",
              "    (1): Linear(in_features=1280, out_features=7, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "num_classes = 7\n",
        "#mobilnet v2\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(p = 0.2, inplace = False),\n",
        "    nn.Linear(\n",
        "        in_features= model.classifier[1].in_features,\n",
        "        out_features = num_classes,\n",
        "        bias = True\n",
        "    )\n",
        ")\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DH4R7D2fB5N",
        "outputId": "3d780c98-a645-41c5-8d04-dda1081d7638"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "97tfzcaee3cK",
        "outputId": "12c41498-7390-435b-c31c-2787dc679f41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "=====================================================================================================================================\n",
              "Layer (type (var_name))                       Input Shape            Output Shape           Param #                Trainable\n",
              "=====================================================================================================================================\n",
              "MobileNetV2 (MobileNetV2)                     [32, 3, 224, 224]      [32, 7]                --                     True\n",
              "├─Sequential (features)                       [32, 3, 224, 224]      [32, 1280, 7, 7]       --                     True\n",
              "│    └─Conv2dNormActivation (0)               [32, 3, 224, 224]      [32, 32, 112, 112]     --                     True\n",
              "│    │    └─Conv2d (0)                        [32, 3, 224, 224]      [32, 32, 112, 112]     864                    True\n",
              "│    │    └─BatchNorm2d (1)                   [32, 32, 112, 112]     [32, 32, 112, 112]     64                     True\n",
              "│    │    └─ReLU6 (2)                         [32, 32, 112, 112]     [32, 32, 112, 112]     --                     --\n",
              "│    └─InvertedResidual (1)                   [32, 32, 112, 112]     [32, 16, 112, 112]     --                     True\n",
              "│    │    └─Sequential (conv)                 [32, 32, 112, 112]     [32, 16, 112, 112]     896                    True\n",
              "│    └─InvertedResidual (2)                   [32, 16, 112, 112]     [32, 24, 56, 56]       --                     True\n",
              "│    │    └─Sequential (conv)                 [32, 16, 112, 112]     [32, 24, 56, 56]       5,136                  True\n",
              "│    └─InvertedResidual (3)                   [32, 24, 56, 56]       [32, 24, 56, 56]       --                     True\n",
              "│    │    └─Sequential (conv)                 [32, 24, 56, 56]       [32, 24, 56, 56]       8,832                  True\n",
              "│    └─InvertedResidual (4)                   [32, 24, 56, 56]       [32, 32, 28, 28]       --                     True\n",
              "│    │    └─Sequential (conv)                 [32, 24, 56, 56]       [32, 32, 28, 28]       10,000                 True\n",
              "│    └─InvertedResidual (5)                   [32, 32, 28, 28]       [32, 32, 28, 28]       --                     True\n",
              "│    │    └─Sequential (conv)                 [32, 32, 28, 28]       [32, 32, 28, 28]       14,848                 True\n",
              "│    └─InvertedResidual (6)                   [32, 32, 28, 28]       [32, 32, 28, 28]       --                     True\n",
              "│    │    └─Sequential (conv)                 [32, 32, 28, 28]       [32, 32, 28, 28]       14,848                 True\n",
              "│    └─InvertedResidual (7)                   [32, 32, 28, 28]       [32, 64, 14, 14]       --                     True\n",
              "│    │    └─Sequential (conv)                 [32, 32, 28, 28]       [32, 64, 14, 14]       21,056                 True\n",
              "│    └─InvertedResidual (8)                   [32, 64, 14, 14]       [32, 64, 14, 14]       --                     True\n",
              "│    │    └─Sequential (conv)                 [32, 64, 14, 14]       [32, 64, 14, 14]       54,272                 True\n",
              "│    └─InvertedResidual (9)                   [32, 64, 14, 14]       [32, 64, 14, 14]       --                     True\n",
              "│    │    └─Sequential (conv)                 [32, 64, 14, 14]       [32, 64, 14, 14]       54,272                 True\n",
              "│    └─InvertedResidual (10)                  [32, 64, 14, 14]       [32, 64, 14, 14]       --                     True\n",
              "│    │    └─Sequential (conv)                 [32, 64, 14, 14]       [32, 64, 14, 14]       54,272                 True\n",
              "│    └─InvertedResidual (11)                  [32, 64, 14, 14]       [32, 96, 14, 14]       --                     True\n",
              "│    │    └─Sequential (conv)                 [32, 64, 14, 14]       [32, 96, 14, 14]       66,624                 True\n",
              "│    └─InvertedResidual (12)                  [32, 96, 14, 14]       [32, 96, 14, 14]       --                     True\n",
              "│    │    └─Sequential (conv)                 [32, 96, 14, 14]       [32, 96, 14, 14]       118,272                True\n",
              "│    └─InvertedResidual (13)                  [32, 96, 14, 14]       [32, 96, 14, 14]       --                     True\n",
              "│    │    └─Sequential (conv)                 [32, 96, 14, 14]       [32, 96, 14, 14]       118,272                True\n",
              "│    └─InvertedResidual (14)                  [32, 96, 14, 14]       [32, 160, 7, 7]        --                     True\n",
              "│    │    └─Sequential (conv)                 [32, 96, 14, 14]       [32, 160, 7, 7]        155,264                True\n",
              "│    └─InvertedResidual (15)                  [32, 160, 7, 7]        [32, 160, 7, 7]        --                     True\n",
              "│    │    └─Sequential (conv)                 [32, 160, 7, 7]        [32, 160, 7, 7]        320,000                True\n",
              "│    └─InvertedResidual (16)                  [32, 160, 7, 7]        [32, 160, 7, 7]        --                     True\n",
              "│    │    └─Sequential (conv)                 [32, 160, 7, 7]        [32, 160, 7, 7]        320,000                True\n",
              "│    └─InvertedResidual (17)                  [32, 160, 7, 7]        [32, 320, 7, 7]        --                     True\n",
              "│    │    └─Sequential (conv)                 [32, 160, 7, 7]        [32, 320, 7, 7]        473,920                True\n",
              "│    └─Conv2dNormActivation (18)              [32, 320, 7, 7]        [32, 1280, 7, 7]       --                     True\n",
              "│    │    └─Conv2d (0)                        [32, 320, 7, 7]        [32, 1280, 7, 7]       409,600                True\n",
              "│    │    └─BatchNorm2d (1)                   [32, 1280, 7, 7]       [32, 1280, 7, 7]       2,560                  True\n",
              "│    │    └─ReLU6 (2)                         [32, 1280, 7, 7]       [32, 1280, 7, 7]       --                     --\n",
              "├─Sequential (classifier)                     [32, 1280]             [32, 7]                --                     True\n",
              "│    └─Dropout (0)                            [32, 1280]             [32, 1280]             --                     --\n",
              "│    └─Linear (1)                             [32, 1280]             [32, 7]                8,967                  True\n",
              "=====================================================================================================================================\n",
              "Total params: 2,232,839\n",
              "Trainable params: 2,232,839\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 9.59\n",
              "=====================================================================================================================================\n",
              "Input size (MB): 19.27\n",
              "Forward/backward pass size (MB): 3419.20\n",
              "Params size (MB): 8.93\n",
              "Estimated Total Size (MB): 3447.39\n",
              "====================================================================================================================================="
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "\n",
        "summary(model=model,\n",
        "        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n",
        "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=22,\n",
        "        row_settings=[\"var_names\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "pbB3FHGiUFg5",
        "outputId": "68f6637f-2a22-4446-937c-a40d3ed66514"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n                These weights improve upon the results of the original paper by using a modified version of TorchVision's\\n                `new training recipe\\n                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\\n            \""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Weights = models.MobileNet_V2_Weights.IMAGENET1K_V2\n",
        "Weights.meta['_docs']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ocZ-jAZUoRY",
        "outputId": "b32988a4-aac8-466f-c477-efee0f441873"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ImageClassification(\n",
              "    crop_size=[224]\n",
              "    resize_size=[232]\n",
              "    mean=[0.485, 0.456, 0.406]\n",
              "    std=[0.229, 0.224, 0.225]\n",
              "    interpolation=InterpolationMode.BILINEAR\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_transform = Weights.transforms()\n",
        "model_transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZNvTfslvZtlS"
      },
      "outputs": [],
      "source": [
        "train_dataset, test_dataset = make_dataset(model_transform)\n",
        "num_classes = len(train_dataset.dataset.classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6o3jidRXXZo0",
        "outputId": "5609b001-2f9c-4547-e19b-c143463e3e18"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Ns-ZAwyYVKXB"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "train_dataloader = DataLoader(\n",
        "    dataset= train_dataset,\n",
        "    batch_size= 32,\n",
        "    shuffle = True,\n",
        "    num_workers= os.cpu_count()\n",
        "\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset = test_dataset,\n",
        "    batch_size = 32,\n",
        "    shuffle = False,\n",
        "    num_workers= os.cpu_count()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "flGSX-q5bChd"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device) -> Tuple[float, float]:\n",
        "  \"\"\"Trains a PyTorch model for a single epoch.\n",
        "\n",
        "  Turns a target PyTorch model to training mode and then\n",
        "  runs through all of the required training steps (forward\n",
        "  pass, loss calculation, optimizer step).\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be trained.\n",
        "    dataloader: A DataLoader instance for the model to be trained on.\n",
        "    loss_fn: A PyTorch loss function to minimize.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A tuple of training loss and training accuracy metrics.\n",
        "    In the form (train_loss, train_accuracy). For example:\n",
        "\n",
        "    (0.1112, 0.8743)\n",
        "  \"\"\"\n",
        "  # Put model in train mode\n",
        "  model.train()\n",
        "\n",
        "  # Setup train loss and train accuracy values\n",
        "  train_loss, train_acc = 0, 0\n",
        "\n",
        "  # Loop through data loader data batches\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "      # Send data to target device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # 1. Forward pass\n",
        "      y_pred = model(X)\n",
        "\n",
        "      # 2. Calculate  and accumulate loss\n",
        "      loss = loss_fn(y_pred, y)\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      # 3. Optimizer zero grad\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 4. Loss backward\n",
        "      loss.backward()\n",
        "\n",
        "      # 5. Optimizer step\n",
        "      optimizer.step()\n",
        "\n",
        "      # Calculate and accumulate accuracy metric across all batches\n",
        "      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  train_loss = train_loss / len(dataloader)\n",
        "  train_acc = train_acc / len(dataloader)\n",
        "  return train_loss, train_acc\n",
        "\n",
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device: torch.device) -> Tuple[float, float]:\n",
        "  \"\"\"Tests a PyTorch model for a single epoch.\n",
        "\n",
        "  Turns a target PyTorch model to \"eval\" mode and then performs\n",
        "  a forward pass on a testing dataset.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be tested.\n",
        "    dataloader: A DataLoader instance for the model to be tested on.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A tuple of testing loss and testing accuracy metrics.\n",
        "    In the form (test_loss, test_accuracy). For example:\n",
        "\n",
        "    (0.0223, 0.8985)\n",
        "  \"\"\"\n",
        "  # Put model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  # Setup test loss and test accuracy values\n",
        "  test_loss, test_acc = 0, 0\n",
        "  all_preds, all_labels = [], []\n",
        "\n",
        "  # Turn on inference context manager\n",
        "  with torch.inference_mode():\n",
        "      # Loop through DataLoader batches\n",
        "      for batch, (X, y) in enumerate(dataloader):\n",
        "          # Send data to target device\n",
        "          X, y = X.to(device), y.to(device)\n",
        "\n",
        "          # 1. Forward pass\n",
        "          test_pred_logits = model(X)\n",
        "\n",
        "          # 2. Calculate and accumulate loss\n",
        "          loss = loss_fn(test_pred_logits, y)\n",
        "          test_loss += loss.item()\n",
        "\n",
        "          # Calculate and accumulate accuracy\n",
        "          test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "          # Store results for later analysis\n",
        "          all_preds.extend(test_pred_labels.cpu().numpy())\n",
        "          all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  test_loss = test_loss / len(dataloader)\n",
        "  test_acc = test_acc / len(dataloader)\n",
        "  return test_loss, test_acc,  all_preds, all_labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0NfIzmhba-kl"
      },
      "outputs": [],
      "source": [
        "from timeit import default_timer as Timer\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device) -> dict[str, list]:\n",
        "\n",
        "  \"\"\"Trains and tests a PyTorch model.\n",
        "\n",
        "  Passes a target PyTorch models through train_step() and test_step()\n",
        "  functions for a number of epochs, training and testing the model\n",
        "  in the same epoch loop.\n",
        "\n",
        "  Calculates, prints and stores evaluation metrics throughout.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be trained and tested.\n",
        "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
        "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
        "    epochs: An integer indicating how many epochs to train for.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A dictionary of training and testing loss as well as training and\n",
        "    testing accuracy metrics. Each metric has a value in a list for\n",
        "    each epoch.\n",
        "    In the form: {train_loss: [...],\n",
        "                  train_acc: [...],\n",
        "                  test_loss: [...],\n",
        "                  test_acc: [...]\n",
        "                  total_time : }\n",
        "    For example if training for epochs=2:\n",
        "                 {train_loss: [2.0616, 1.0537],\n",
        "                  train_acc: [0.3945, 0.3945],\n",
        "                  test_loss: [1.2641, 1.5706],\n",
        "                  test_acc: [0.3400, 0.2973]\n",
        "                  total_time : 300s}\n",
        "  \"\"\"\n",
        "  # Create empty results dictionary\n",
        "  results = {\n",
        "      \"train_loss\": [],\n",
        "      \"train_acc\": [],\n",
        "      \"test_loss\": [],\n",
        "      \"test_acc\": [],\n",
        "      'all_preds' : [],\n",
        "      'all_labels' : [],\n",
        "  }\n",
        "\n",
        "\n",
        "  start_time = Timer()\n",
        "  # Loop through training and testing steps for a number of epochs\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "      train_loss, train_acc = train_step(model=model,\n",
        "                                          dataloader=train_dataloader,\n",
        "                                          loss_fn=loss_fn,\n",
        "                                          optimizer=optimizer,\n",
        "                                          device=device)\n",
        "      test_loss, test_acc,all_preds, all_labels  = test_step(model=model,\n",
        "          dataloader=test_dataloader,\n",
        "          loss_fn=loss_fn,\n",
        "          device=device)\n",
        "\n",
        "      # Print out what's happening\n",
        "      print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f}\"\n",
        "      )\n",
        "\n",
        "      # Update results dictionary\n",
        "      results[\"train_loss\"].append(train_loss)\n",
        "      results[\"train_acc\"].append(train_acc)\n",
        "      results[\"test_loss\"].append(test_loss)\n",
        "      results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "  end_time = Timer()\n",
        "\n",
        "  results['total_time'] = end_time - start_time\n",
        "  results['all_preds'] = all_preds\n",
        "  results['all_labels'] = all_labels\n",
        "  # Return the filled results at the end of the epochs\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BWlbnESXQz6",
        "outputId": "d35f61fc-aad5-4201-911c-beeb3927f12c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'surprise': 5920,\n",
              " 'fear': 5920,\n",
              " 'angry': 5920,\n",
              " 'neutral': 8166,\n",
              " 'sad': 6535,\n",
              " 'disgust': 5920,\n",
              " 'happy': 11398}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "data_root = f'{root_path}/processed_data'\n",
        "counts = {}\n",
        "\n",
        "for cls in os.listdir(data_root):\n",
        "    path = os.path.join(data_root, cls)\n",
        "    if os.path.isdir(path):\n",
        "        counts[cls] = len(os.listdir(path))\n",
        "        # print(f\"{cls}: {len(os.listdir(path))}\")\n",
        "\n",
        "counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0qb6o6VhYOUV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class_names = train_dataset.dataset.classes\n",
        "weights = torch.tensor([1.0 / counts[c] for c in class_names], dtype=torch.float)\n",
        "\n",
        "model.to(device)\n",
        "weight=weights.to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(weight=weights.to(device))\n",
        "optimizer = torch.optim.Adam(model.parameters(),  lr=1e-3, weight_decay=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228,
          "referenced_widgets": [
            "fdbdd51b0ac44c5cacb9e991f229ca25",
            "7f07be70208544929805c28dc019ae09",
            "e7427da71df944cdb87f0ba87d451a06",
            "c6a4799185e34cda92dbe5ebaa317e74",
            "98084e401b424bf8b95f1cf171ce5c4c",
            "a378fd807e03488c98186404b77d9a08",
            "15210d8ad5534c8a8ae47a04d1f171ba",
            "ce876a30b31f413da82eb0ba3cc0528b",
            "ece1ff6bb5aa45eea2ef0cb3a40a733c",
            "775e702869094cdfa4390c6bd0dee477",
            "965c59febe1249048c15fcbb2ba41534"
          ]
        },
        "id": "ssJ0O-dSbRx5",
        "outputId": "782bc56c-8731-42db-9ca7-ac20ab4e8ed8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fdbdd51b0ac44c5cacb9e991f229ca25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 | train_loss: 1.1952 | train_acc: 0.5684 | test_loss: 1.0576 | test_acc: 0.6247\n",
            "Epoch: 2 | train_loss: 1.0045 | train_acc: 0.6413 | test_loss: 0.9722 | test_acc: 0.6594\n",
            "Epoch: 3 | train_loss: 0.9365 | train_acc: 0.6667 | test_loss: 0.9523 | test_acc: 0.6518\n",
            "Epoch: 4 | train_loss: 0.8878 | train_acc: 0.6865 | test_loss: 0.9031 | test_acc: 0.6752\n",
            "Epoch: 5 | train_loss: 0.8561 | train_acc: 0.6983 | test_loss: 0.8978 | test_acc: 0.6764\n",
            "Epoch: 6 | train_loss: 0.8219 | train_acc: 0.7078 | test_loss: 0.8837 | test_acc: 0.6842\n",
            "Epoch: 7 | train_loss: 0.7899 | train_acc: 0.7192 | test_loss: 0.8714 | test_acc: 0.6871\n",
            "Epoch: 8 | train_loss: 0.7616 | train_acc: 0.7308 | test_loss: 0.8857 | test_acc: 0.6784\n",
            "Epoch: 9 | train_loss: 0.7358 | train_acc: 0.7362 | test_loss: 0.8537 | test_acc: 0.6957\n",
            "Epoch: 10 | train_loss: 0.7091 | train_acc: 0.7478 | test_loss: 0.8420 | test_acc: 0.6994\n"
          ]
        }
      ],
      "source": [
        "results = train(\n",
        "    model = model,\n",
        "    train_dataloader = train_dataloader,\n",
        "    test_dataloader = test_dataloader,\n",
        "    optimizer = optimizer,\n",
        "    loss_fn = loss_fn,\n",
        "    epochs = 10,\n",
        "    device = device\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_wN2Ddjdgp1",
        "outputId": "6deea94d-1d8b-4bf4-942e-1137e80ae610"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "32.0"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results['total_time'] // 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRB4a5bVJU6e",
        "outputId": "3efa3f09-28de-4346-f789-2b8c911067be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "angry\n"
          ]
        }
      ],
      "source": [
        "dimg = Image.open(f'{root_path}/processed_data/angry/angry_00000.jpg')\n",
        "\n",
        "img_tensor = model_transform(img)\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "  y_pred = model(img_tensor.unsqueeze(dim = 0).to(device))\n",
        "  pred_class = class_names[y_pred.argmax(dim = 1)]\n",
        "  print(pred_class)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get real time prediction in colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "fuWvzhXj3vOQ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Javascript, display\n",
        "from google.colab.output import eval_js\n",
        "import cv2\n",
        "import numpy as np\n",
        "from base64 import b64decode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "JAYJqHqtsdbG"
      },
      "outputs": [],
      "source": [
        "def take_photo(quality=0.8):\n",
        "    js = Javascript('''\n",
        "      async function takePhoto(quality) {\n",
        "        const div = document.createElement('div');\n",
        "        const capture = document.createElement('button');\n",
        "        capture.textContent = '📸 Capture';\n",
        "        div.appendChild(capture);\n",
        "        document.body.appendChild(div);\n",
        "\n",
        "        const video = document.createElement('video');\n",
        "        video.style.display = 'block';\n",
        "        const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "        document.body.appendChild(video);\n",
        "        video.srcObject = stream;\n",
        "        await video.play();\n",
        "\n",
        "        // Wait for button press\n",
        "        await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "        const canvas = document.createElement('canvas');\n",
        "        canvas.width = video.videoWidth;\n",
        "        canvas.height = video.videoHeight;\n",
        "        canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "        stream.getTracks().forEach(track => track.stop());\n",
        "        video.remove();\n",
        "        div.remove();\n",
        "\n",
        "        const dataUrl = canvas.toDataURL('image/jpeg', quality);\n",
        "        return dataUrl;\n",
        "      }\n",
        "      takePhoto(%f);\n",
        "    ''' % quality)\n",
        "\n",
        "    display(js)\n",
        "    data = eval_js('takePhoto(%f)' % quality)\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    arr = np.frombuffer(binary, dtype=np.uint8)\n",
        "    return cv2.imdecode(arr, cv2.IMREAD_COLOR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "BlIaZOgmIEdq"
      },
      "outputs": [],
      "source": [
        "from torchvision import  transforms\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoFac0lpA_z2"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "import time\n",
        "from IPython.display import display, HTML, clear_output\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "display(HTML('''\n",
        "<button onclick=\"window.colab_stop_loop = true\">Stop Capture</button>\n",
        "<script>window.colab_stop_loop = false;</script>\n",
        "'''))\n",
        "\n",
        "while True:\n",
        "    # Check stop flag from JS\n",
        "    stop = output.eval_js(\"window.colab_stop_loop\")\n",
        "    if stop == 'true':\n",
        "        print(\"Stopped by user.\")\n",
        "        break\n",
        "\n",
        "    # Take photo (NumPy array)\n",
        "    frame = take_photo()  # assumes this is defined\n",
        "    cv2_imshow(frame)\n",
        "\n",
        "    # Preprocess and predict\n",
        "    input_tensor = model_transform(Image.fromarray(frame)).unsqueeze(dim = 0).to(device)\n",
        "  # (1,C,H,W)\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        y_pred = model(input_tensor)\n",
        "        pred_class_name = class_names[y_pred.argmax(dim=1).item()]\n",
        "\n",
        "    print('Predicted Emotion:', pred_class_name)\n",
        "\n",
        "    # Small delay to avoid flooding\n",
        "    time.sleep(0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save the model\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "model_path = Path('Models')\n",
        "\n",
        "model_path.mkdir(\n",
        "    parents=True,\n",
        "    exist_ok=True\n",
        ")\n",
        "\n",
        "model_name = 'mobilnetv2_emotion_detection'\n",
        "\n",
        "model_save_path = model_path / model_name\n",
        "\n",
        "\n",
        "torch.save({\n",
        "    'model' : model.state_dict(),\n",
        "    'class_names' : class_names,\n",
        "    'img_size' : 232,\n",
        "}, f = model_save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get real time prediction in local device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#load the model in local device\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 1️⃣ Recreate the model architecture exactly\n",
        "num_classes = 7\n",
        "model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.2, inplace=False),\n",
        "    nn.Linear(\n",
        "        in_features=model.classifier[1].in_features,\n",
        "        out_features=num_classes,\n",
        "        bias=True\n",
        "    )\n",
        ")\n",
        "model.to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint = torch.load(\"/Users/akash/All code/python/random/mobilnetv2_emotion_detection\", map_location=device) #path to the saved model\n",
        "model.load_state_dict(checkpoint[\"model\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_classes = checkpoint['class_names']\n",
        "num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Preprocessing transform (same as training)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load face detector\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Set model to eval mode\n",
        "model.eval()\n",
        "\n",
        "# Open webcam\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open webcam\")\n",
        "else:\n",
        "    print(\"Press 'q' to quit\")\n",
        "    \n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        \n",
        "        if not ret:\n",
        "            print(\"Error: Can't receive frame\")\n",
        "            break\n",
        "        \n",
        "        # Convert to grayscale for face detection\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        \n",
        "        # Detect faces\n",
        "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "        \n",
        "        # Process each detected face\n",
        "        for (x, y, w, h) in faces:\n",
        "            # Draw rectangle around face\n",
        "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "            \n",
        "            # Extract face region\n",
        "            face_roi = frame[y:y+h, x:x+w]\n",
        "            \n",
        "            # Convert BGR to RGB\n",
        "            face_rgb = cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB)\n",
        "            \n",
        "            # Convert to PIL Image\n",
        "            face_pil = Image.fromarray(face_rgb)\n",
        "            \n",
        "            # Preprocess\n",
        "            face_tensor = transform(face_pil).unsqueeze(0).to(device)\n",
        "            \n",
        "            # Predict emotion\n",
        "            with torch.inference_mode():\n",
        "                output = model(face_tensor)\n",
        "                pred_idx = output.argmax(dim=1).item()\n",
        "                if pred_idx != 1:\n",
        "                    emotion = num_classes[pred_idx]\n",
        "                    confidence = torch.softmax(output, dim=1)[0][pred_idx].item()\n",
        "            \n",
        "            # Display emotion on frame\n",
        "            text = f\"{emotion}: {confidence:.2f}\"\n",
        "            cv2.putText(frame, text, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "        \n",
        "        # Show frame\n",
        "        cv2.imshow('Real-time Emotion Detection - Press Q to quit', frame)\n",
        "        \n",
        "        # Press 'q' to exit\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "    \n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    print(\"Webcam released\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 8376054,
          "sourceId": 13215089,
          "sourceType": "datasetVersion"
        }
      ],
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "15210d8ad5534c8a8ae47a04d1f171ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "775e702869094cdfa4390c6bd0dee477": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f07be70208544929805c28dc019ae09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a378fd807e03488c98186404b77d9a08",
            "placeholder": "​",
            "style": "IPY_MODEL_15210d8ad5534c8a8ae47a04d1f171ba",
            "value": "100%"
          }
        },
        "965c59febe1249048c15fcbb2ba41534": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98084e401b424bf8b95f1cf171ce5c4c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a378fd807e03488c98186404b77d9a08": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6a4799185e34cda92dbe5ebaa317e74": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_775e702869094cdfa4390c6bd0dee477",
            "placeholder": "​",
            "style": "IPY_MODEL_965c59febe1249048c15fcbb2ba41534",
            "value": " 10/10 [32:14&lt;00:00, 188.82s/it]"
          }
        },
        "ce876a30b31f413da82eb0ba3cc0528b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7427da71df944cdb87f0ba87d451a06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce876a30b31f413da82eb0ba3cc0528b",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ece1ff6bb5aa45eea2ef0cb3a40a733c",
            "value": 10
          }
        },
        "ece1ff6bb5aa45eea2ef0cb3a40a733c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fdbdd51b0ac44c5cacb9e991f229ca25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f07be70208544929805c28dc019ae09",
              "IPY_MODEL_e7427da71df944cdb87f0ba87d451a06",
              "IPY_MODEL_c6a4799185e34cda92dbe5ebaa317e74"
            ],
            "layout": "IPY_MODEL_98084e401b424bf8b95f1cf171ce5c4c"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
